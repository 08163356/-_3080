---
title: emmc知识问题想法汇总
date: 2024-01-29 04:23:39
permalink: /pages/884275/
categories:
  - 技术
  - 操作系统
  - Linux
tags:
  - 
author: 
  name: aXing
  link: https://github.com/08163356
---

知识点

linux系统下emmc之ext4文件系统及jdb2分析篇

​            驱动软件所  初德进

**关键词**：ext4、jdb2、super block

**一、**  **基础知识**

**1.1**  **ext4****新特性**

Ext4（The fourth extended file system）,是专门为 Linux 开发的原始的扩展文件系统（ext 或 extfs）的第四版。 Linux kernel 自 2.6.28 开始正式支持新的文件系统 Ext4。 Ext4 是 Ext3 的改进版，修改了 Ext3 中部分重要的[数据结构](http://baike.baidu.com/view/9900.htm)。ext4文件系统可支持最高1 [Exbibyte](http://zh.wikipedia.org/wiki/Exbibyte)的分区与最大16 [Tebibyte](http://zh.wikipedia.org/wiki/Tebibyte)的文件。其主要特点包括：

1)  Extents
<!-- more -->

Ext4引进了[Extent](http://zh.wikipedia.org/wiki/Extent_(檔案系統))文件存储方式，以取代ext2/3使用的[block mapping](http://zh.wikipedia.org/w/index.php?title=Block_(data_storage)&action=edit&redlink=1)方式。Extent指的是一连串的连续实体block，这种方式可以增加大型文件的效率并减少分裂文件。ext4支持的单一Extent，在单一block为4KB的系统中最高可达128MB。单一inode中可存储4笔Extent；超过四笔的Extent会以Htree方式被索引。

2)  向下兼容

Ext4向下兼容于[ext3](http://zh.wikipedia.org/wiki/Ext3)与[ext2](http://zh.wikipedia.org/wiki/Ext2)，因此可以将ext3和ext2的文件系统挂载为ext4分区。由于某些ext4的新功能可以直接运用在ext3和ext2上，直接挂载即可提升少许性能。ext3文件系统可以部分[向上兼容](http://zh.wikipedia.org/w/index.php?title=向上相容&action=edit&redlink=1)于ext4（也就是说ext4文件系统可以被挂载为ext3分区）。然而若是使用到Extent技术的ext4将无法被挂载为ext3。

3)  预留空间

Ext4允许对一文件预先保留磁盘空间。目前大多数文件系统做到这点的方式是直接产生一个填满0的文件；ext4和[XFS](http://zh.wikipedia.org/wiki/XFS)可以使用Linux核心中的一个新的系统调用“fallocate()”取得足够的预留空间。

4)  延迟取得空间

Ext4使用一种称为[allocate-on-flush](http://zh.wikipedia.org/w/index.php?title=Allocate-on-flush&action=edit&redlink=1)的方式，可以在数据将被写入磁盘（sync）前才开始取得空间；大多数文件系统会在之前便取得需要的空间。这种方式可以增加性能并减少文件分散程度。

5)  突破32000子目录限制

Ext3的一个目录下最多只能有32000个子目录。ext4的子目录最高可达64000，且使用“dir_nlink”功能后可以达到更高（虽然父目录的link count会停止增加）。为了避免性能受到大量目录的影响，ext4默认打开                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          索引功能。该功能已经实现于Linux核心2.6.23版。

6)  日志校验和                                      

Ext4使用[校验和](http://zh.wikipedia.org/wiki/校验和)特性来提高文件系统可靠性，因为日志是磁盘上被读取最频繁的部分之一。这个特性还有一个好处就是可以安全地避免日志处理时磁盘I/O的等待，而稍微提高一些性能。

7)  在线磁盘整理

对于在线[磁盘整理](http://zh.wikipedia.org/w/index.php?title=磁盘整理&action=edit&redlink=1)工具有许多草案，但是这些草案都没有被包含在主流的[内核](http://zh.wikipedia.org/wiki/内核)当中。即使Ext4包含有许多避免[磁盘碎片](http://zh.wikipedia.org/wiki/磁盘碎片)的技术，但是磁盘碎片还是难免会在一个长时间使用过的[文件系统](http://zh.wikipedia.org/wiki/文件系统)中存在。Ext4将会有一个具有磁盘整理功能的工具。

8)  快速文件系统检查

Ext4将未使用的区块标记在inode当中，这样可以使诸如e2fsck之类的工具在磁盘检查时将这些区块完全跳过，而节约大量的文件系统检查的时间。这个特性已经在2.6.24版本的[Linux](http://zh.wikipedia.org/wiki/Linux)[内核](http://zh.wikipedia.org/wiki/内核)中实现。

**2.1** **ext4****组成**

文件系统是特殊的数据分层存储结构，它包含文件、目录和相关的控制信息。

为了降低开销，提高效率，Ext4 将存储设备划分为一批逻辑块。通常块的大小为4KB，计算方法为 2 ^ (10 + sb.s_log_block_size) 字节。块大小的最小值为1024，最大值为65536。当我们使用 mke2fs 对磁盘分区进行格式化时，终端的打印信息中包含类似这样的一行 Block size=4096 (log=2) 内容，说明块大小为 4096，即 2 的 10+2 次方。当然，你现在也可以通过 mke2fs -n 来查看。

除了块以外，Ext4文件系统还被划分为一系列块组。为了降低碎片引起的性能瓶颈，块分配器尽量保持每个文件的块位于同一组中，以减少寻道时间。块组的大小记录在 sb.s_blocks_per_group 中，当然也可以通过 8×block_size_in_bytes 的方法计算出来。当然，它也有最大值为 65536 的限制（事实上，代码中将其限制为 65536 - 8）。以默认的 4KB 块大小为例，每个块组就包含 8×4096＝32768 个块，大小为 128MB。块组的数量由设备大小和块组大小决定。

值得注意的是，Ext4 中所有字段都是按照小端规则写入的，而 jbd2（日志）中的字段都是按照大端规则写入的。下面是一个标准块组的大致结构：

| 块组0填充 | ext4超级块 | 组描述信息 | 预留的GDT块 | 数据块位映像 | 索引节点位映像 | 索引表 | 数据块   |
| --------- | ---------- | ---------- | ----------- | ------------ | -------------- | ------ | -------- |
| 1024 字节 | 1 个块     | 多个块     | 多个块      | 1个块        | 1个块          | 多个块 | 更多的块 |

介于块组0的特殊性，最开始的1024个字节并未使用，而是留给了x86引导区等等。因此，块组0的超级块起始于1024个字节的偏移之后。如果因为某些原因，块大小只有1024字节，那么块0就会被标记为已用，超级块从块1开始。而其他的块组，就没有这1024字节的填充了。

Ext4 驱动主要依据块组0中的超级块和组描述工作。同时，在磁盘的一些其他块组中还保存着它们的冗余副本，以防磁盘开头部位损坏。当然，并非所有的块组都存有一个副本（后面会介绍一些细节）。没有副本的块组由数据块位映像开始。注意，文件系统刚初始化时，会在组描述之后，数据块位映像之前，分配“预留的GDT块”空间，以便将来文件系统的扩展。默认情况下，一个文件系统允许在原有文件系统的基础上按照1024的倍数进行扩增。

我们以实际设备为例，直观的看看文件系统在emmc中的形态。下面是MT5506平台的分区信息：

![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image002.jpg)

system分区的超级块,使用dumpe2fs命令读出。

dumpe2fs 1.42.8 (20-Jun-2013)

Filesystem volume name:  <none>

Last mounted on:      /system

Filesystem UUID:      57f8f4bc-abf4-655f-bf67-946fc0f9f25b

Filesystem magic number:  0xEF53

Filesystem revision #:   1 (dynamic)

Filesystem features:    has_journal resize_inode filetype needs_recovery extent sparse_super large_file

Filesystem flags:     unsigned_directory_hash 

Default mount options:   (none)

Filesystem state:     clean

Errors behavior:      Remount read-only

Filesystem OS type:    Linux

Inode count:        128000

Block count:        512000

Reserved block count:   0

Free blocks:        312090

Free inodes:        126628

First block:        0

Block size:        4096

Fragment size:       4096

Reserved GDT blocks:    127

Blocks per group:     32768

Fragments per group:    32768

Inodes per group:     8000

Inode blocks per group:  500

Last mount time:      Tue Aug  6 01:37:26 2013

Last write time:      Tue Aug  6 01:37:26 2013

Mount count:        25

Maximum mount count:    -1

Last checked:       Thu Jan  1 00:00:00 1970

Check interval:      0 (<none>)

Lifetime writes:      1084 kB

Reserved blocks uid:    0 (user unknown)

Reserved blocks gid:    0 (group unknown)

First inode:        11

Inode size:        256

Required extra isize:   28

Desired extra isize:    28

Journal inode:       8

Default directory hash:  tea

Journal backup:      inode blocks

Journal features:     (none)

Journal size:       31M

Journal length:      8000

Journal sequence:     0x00000078

Journal start:       1

 

Group 0: (Blocks 0-32767)

 Primary superblock at 0, Group descriptors at 1-1

 Reserved GDT blocks at 2-128

 Block bitmap at 129 (+129), Inode bitmap at 130 (+130)

 Inode table at 131-630 (+131)

 0 free blocks, 6631 free inodes, 65 directories

 Free blocks: 

 Free inodes: 1369, 1371-8000

Group 1: (Blocks 32768-65535)

 Backup superblock at 32768, Group descriptors at 32769-32769

 Reserved GDT blocks at 32770-32896

 Block bitmap at 32897 (+129), Inode bitmap at 32898 (+130)

 Inode table at 32899-33398 (+131)

 1 free blocks, 8000 free inodes, 0 directories

 Free blocks: 65535

 Free inodes: 8001-16000

Group 2: (Blocks 65536-98303)

 Block bitmap at 65536 (+0), Inode bitmap at 65537 (+1)

 Inode table at 65538-66037 (+2)

 1 free blocks, 8000 free inodes, 0 directories

 Free blocks: 98303

 Free inodes: 16001-24000

Group 3: (Blocks 98304-131071)

 Backup superblock at 98304, Group descriptors at 98305-98305

 Reserved GDT blocks at 98306-98432

 Block bitmap at 98433 (+129), Inode bitmap at 98434 (+130)

 Inode table at 98435-98934 (+131)

 1 free blocks, 8000 free inodes, 0 directories

 Free blocks: 131071

 Free inodes: 24001-32000

Group 4: (Blocks 131072-163839)

 Block bitmap at 131072 (+0), Inode bitmap at 131073 (+1)

 Inode table at 131074-131573 (+2)

 1 free blocks, 8000 free inodes, 0 directories

 Free blocks: 163839

 Free inodes: 32001-40000

Group 5: (Blocks 163840-196607)

 Backup superblock at 163840, Group descriptors at 163841-163841

 Reserved GDT blocks at 163842-163968

 Block bitmap at 163969 (+129), Inode bitmap at 163970 (+130)

 Inode table at 163971-164470 (+131)

 2997 free blocks, 8000 free inodes, 0 directories

 Free blocks: 193611-196607

 Free inodes: 40001-48000

Group 6: (Blocks 196608-229375)

 Block bitmap at 196608 (+0), Inode bitmap at 196609 (+1)

 Inode table at 196610-197109 (+2)

 32266 free blocks, 8000 free inodes, 0 directories

 Free blocks: 197110-229375

 Free inodes: 48001-56000

Group 7: (Blocks 229376-262143)

 Backup superblock at 229376, Group descriptors at 229377-229377

 Reserved GDT blocks at 229378-229504

 Block bitmap at 229505 (+129), Inode bitmap at 229506 (+130)

 Inode table at 229507-230006 (+131)

 32137 free blocks, 8000 free inodes, 0 directories

 Free blocks: 230007-262143

 Free inodes: 56001-64000

Group 8: (Blocks 262144-294911)

 Block bitmap at 262144 (+0), Inode bitmap at 262145 (+1)

 Inode table at 262146-262645 (+2)

 32266 free blocks, 8000 free inodes, 0 directories

 Free blocks: 262646-294911

 Free inodes: 64001-72000

Group 9: (Blocks 294912-327679)

 Backup superblock at 294912, Group descriptors at 294913-294913

 Reserved GDT blocks at 294914-295040

 Block bitmap at 295041 (+129), Inode bitmap at 295042 (+130)

 Inode table at 295043-295542 (+131)

 32137 free blocks, 8000 free inodes, 0 directories

 Free blocks: 295543-327679

 Free inodes: 72001-80000

Group 10: (Blocks 327680-360447)

 Block bitmap at 327680 (+0), Inode bitmap at 327681 (+1)

 Inode table at 327682-328181 (+2)

 32266 free blocks, 8000 free inodes, 0 directories

 Free blocks: 328182-360447

 Free inodes: 80001-88000

Group 11: (Blocks 360448-393215)

 Block bitmap at 360448 (+0), Inode bitmap at 360449 (+1)

 Inode table at 360450-360949 (+2)

 32266 free blocks, 8000 free inodes, 0 directories

 Free blocks: 360950-393215

 Free inodes: 88001-96000

Group 12: (Blocks 393216-425983)

 Block bitmap at 393216 (+0), Inode bitmap at 393217 (+1)

 Inode table at 393218-393717 (+2)

 32266 free blocks, 8000 free inodes, 0 directories

 Free blocks: 393718-425983

 Free inodes: 96001-104000

Group 13: (Blocks 425984-458751)

 Block bitmap at 425984 (+0), Inode bitmap at 425985 (+1)

 Inode table at 425986-426485 (+2)

 31241 free blocks, 7997 free inodes, 0 directories

 Free blocks: 427511-458751

 Free inodes: 104004-112000

Group 14: (Blocks 458752-491519)

 Block bitmap at 458752 (+0), Inode bitmap at 458753 (+1)

 Inode table at 458754-459253 (+2)

 32266 free blocks, 8000 free inodes, 0 directories

 Free blocks: 459254-491519

 Free inodes: 112001-120000

Group 15: (Blocks 491520-511999)

 Block bitmap at 491520 (+0), Inode bitmap at 491521 (+1)

 Inode table at 491522-492021 (+2)

 19978 free blocks, 8000 free inodes, 0 directories

 Free blocks: 492022-511999

 Free inodes: 120001-12800

  可以看到system分区分为16个组，/system分区的起始地址是0x2500000,由于我们使用的emmc容量是16G，所以给emmc发送的地址是块地址，即0x12800。在uboot中，使用命令：

mmc read 0 0x200000 0x12800 0x100

读出文件系统信息到内存的0x200000中，然后使用md 0x200000 来查看得到的数据。从实际的数据来看，块组0的第0个块存放的是文件系统超级块，而且前1024个字节数据为0，符合上文的描述。ext4_super_block结构体表示的超级块信息是：

00200400: 0001f400 0007d000 00000000 0004c31a   ................

00200410: 0001eea4 00000000 00000002 00000002   ................

00200420: 00008000 00008000 00001f40 5200884f   ........@...O..R

00200430:5200884f ffff001a0001ef53 00000002   O..R....S.......     magic

00200440: 00000000 00000000 00000000 00000001   ................

00200450: 00000000 0000000b 00000100 00000014   ................

00200460: 00000046 00000003 bcf4f857 5f65f4ab   F.......W.....e_

00200470: 6f9467bf 5bf2f9c0 00000000 00000000   .g.o...[........

00200480: 00000000 00000000 7379732f 006d6574   ......../system.

00200490: 00000000 00000000 00000000 00000000   ................

002004a0: 00000000 00000000 00000000 00000000   ................

002004b0: 00000000 00000000 00000000 00000000   ................

002004c0: 00000000 00000000 00000000 007f0000   ................

002004d0: 00000000 00000000 00000000 00000000   ................

002004e0: 00000008 00000000 00000000 00000000   ................

002004f0: 00000000 00000000 00000000 00200102   .............. .

00200500: 00000000 00000000 00000000 0001f30a   ................

00200510: 00000003 00000000 00000000 00001f40   ............@...

00200520: 00000277 00000000 00000000 00000000   w...............

00200530: 00000000 00000000 00000000 00000000   ................

00200540: 00000000 00000000 00000000 00000000   ................

00200550: 00000000 00000000 00000000 001c001c   ................

00200560: 00000002 00000000 00000000 00000000   ................

00200570: 00000000 00000000 00000468 00000000   ........h.......

00200580: 00000000 00000000 00000000 00000000   ................

00200590: 00000000 00000000 00000000 00000000   ................

002005a0: 00000000 00000000 00000000 00000000   ................

002005b0: 00000000 00000000 00000000 00000000   ................

002005c0: 00000000 00000000 00000000 00000000   ................

002005d0: 00000000 00000000 00000000 00000000   ................

002005e0: 00000000 00000000 00000000 00000000   ................

002005f0: 00000000 00000000 00000000 00000000   ................

 

https://ext4.wiki.kernel.org/index.php/Ext4_Disk_Layout这个网页全面描述了ext4文件系统在硬盘中的数据组织结构。下面来试试看看一个文件的信息，所用平台为mt5506。

shell@android:/system # busybox stat build.prop                 

File: build.prop

 Size: 2678       Blocks: 8      IO Block: 4096  regular file

Device: b306h/45830d   Inode: 358     Links: 1

Access: (0644/-rw-r--r--)  Uid: (   0/ UNKNOWN)  Gid: (   0/ UNKNOWN)

Access: 2013-07-29 10:58:58.000000000

Modify: 2013-07-29 10:58:58.000000000

Change: 2013-07-29 10:58:58.000000000

由dumpe2fs命令得到的超级块信息来看，每个块组有8000个inodes，每个inodes大小为256字节，每个块组需要有500个块来存放inodes，这里的块大小为4K，所以这个文件位于

第0个块组，其描述为：

Group 0: (Blocks 0-32767)

 Primary superblock at 0, Group descriptors at 1-1

 Reserved GDT blocks at 2-128

 Block bitmap at 129 (+129), Inode bitmap at 130 (+130)

 Inode table at 131-630 (+131)

 0 free blocks, 6631 free inodes, 65 directories

 Free blocks: 

 Free inodes: 1369, 1371-8000

第358个inode位于第358/16+131=153个块中。。emmc一块的大小为512字节，因此这个文件实际位于（358/16+131）*8+0x12800-1=0x12cca

可以总结出公式：

inode物理块号=(inode/16+inode起始块号)*8+分区起始物理块号-1

ext4支持3种DATA模式，用来区分记录journal的行为。文件在ext4中分两部分存储，一部分是文件的metadata，另一部分是data。metadata和data的操作日志journal也是分开管理的。你可以让ext4记录metadata的journal，而不记录data的journal，这取决于mount ext4时的data参数。

1)    data=journal    

在将data写入文件系统前，必须等待metadata和data的journal已经写入磁盘了。性能最差，并且不支持文件操作的delalloc，O_DIRECT flag (参考 man open)。当调用fsync时，文件系统的操作包含如下顺序：

fsync(data journal) -> fsync(metadata journal) -> fsync(data) -> fsync(metadata)

lock metadata long time between fsync(metadata journal) and fsync(metadata)

2)    data=ordered   

这个模式不记录data的journal，只记录metadata的journal日志，但是在写metadata的journal前，必须先确保data已经落盘。当调用fsync时，文件系统的操作包含如下顺序：

fsync(metadata journal) -> fsync(data)(确保data先落盘) -> fsync(metadata)

lock metadata long time between fsync(metadata journal) and fsync(metadata)

3)     data=writeback       

不记录data journal，仅记录metadata journal。并且不保证data比metadata先落盘。当调用fsync时，文件系统的操作包含如下顺序：

fsync(metadata journal) -> fsync(metadata)

 

另外需要注意metadata的操作在单个ext4文件系统中是串行的，所以如果某个用户的metadata操作堵塞了的话，会影响所有人操作同一个文件系统的metadata。即使使用writeback模式，也会有这样的情况发生，例如某个用户疯狂的写metadata的（例如大批量的创建小文件，调用fsync）。

 

典型的例子：

fsync(data)这一步如果很慢，会导致其他人写metadata等待的现象(写metadata包括很多，例如创建新文件，读写方式打开文件（改变文件大小）)。

查看进程栈：

10249正在fsync：
 ![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image004.jpg)10255正在创建新文件，被堵塞：
 ![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image006.jpg)

 [ 解决思路 ]

\1. 通过调整内核刷dirty page的比例和唤醒时间，可以让内核频繁的收回脏页，从而降低以上问题出现的概率，不过这种做法有利有弊。

相关的参数：
 ![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image008.jpg)相关进程：
 ![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image010.jpg)

系统缓存相关的几个内核参数 (还有2个是指定bytes的，含义和ratio差不多)：

1)     /proc/sys/vm/dirty_background_ratio

该文件表示脏数据到达系统整体内存的百分比，此时触发pdflush进程把脏数据写回磁盘。

缺省设置：10。当用户调用write时，如果发现系统中的脏数据大于这阈值（或dirty_background_bytes ），会触发pdflush进程去写脏数据，但是用户的write调用会立即返回，无需等待。pdflush刷脏页的标准是让脏页降低到该阈值以下。即使cgroup限制了用户进程的IOPS，也无所谓。

2)     /proc/sys/vm/dirty_expire_centisecs

该文件表示如果脏数据在内存中驻留时间超过该值，pdflush进程在下一次将把这些数据写回磁盘。缺省设置：3000（1/100秒）

3)     /proc/sys/vm/dirty_ratio

该文件表示如果进程产生的脏数据到达系统整体内存的百分比，此时用户进程自行把脏数据写回磁盘。缺省设置：40。当用户调用write时，如果发现系统中的脏数据大于这阈值（或dirty_bytes ），需要自己把脏数据刷回磁盘，降低到这个阈值以下才返回。

注意，此时如果cgroup限制了用户进程的IOPS，那就悲剧了。

4)     /proc/sys/vm/dirty_writeback_centisecs

该文件表示pdflush进程的唤醒间隔，周期性把超过dirty_expire_centisecs时间的脏数据写回磁盘。缺省设置：500（1/100秒）

 系统一般在下面三种情况下回写dirty页:

1)     定时方式: 定时回写是基于这样的原则:/proc/sys/vm/dirty_writeback_centisecs的值表示多长时间会启动回写线程,由这个定时器启动的回写线程只回写在内存中为dirty时间超过(/proc/sys/vm/dirty_expire_centisecs / 100)秒的页(这个值默认是3000,也就是30秒),一般情况下dirty_writeback_centisecs的值是500,也就是5秒,所以默认情况下系统会5秒钟启动一次回写线程,把dirty时间超过30秒的页回写,要注意的是,这种方式启动的回写线程只回写超时的dirty页，不会回写没超时的dirty页,可以通过修改/proc中的这两个值，细节查看内核函数wb_kupdate。

2)     内存不足的时候: 这时并不将所有的dirty页写到磁盘,而是每次写大概1024个页面,直到空闲页面满足需求为止

3)     写操作时发现脏页超过一定比例: 当脏页占系统内存的比例超过/proc/sys/vm/dirty_background_ratio 的时候,write系统调用会唤醒pdflush回写dirty page,直到脏页比例低于/proc/sys/vm/dirty_background_ratio,但write系统调用不会被阻塞,立即返回.

   vm.dirty_background_ratio 是内存可以填充“脏数据”的百分比。这些“脏数据”在稍后是会写入磁盘的，pdflush/flush/kdmflush这些后台进程会稍后清理脏数据。举一个例子，我有32G内存，那么有3.2G的内存可以待着内存里，超过3.2G的话就会有后来进程来清理它。

vm.dirty_ratio 是绝对的脏数据限制，内存里的脏数据百分比不能超过这个值。如果脏数据超过这个数量，新的IO请求将会被阻挡，直到脏数据被写进磁盘。这是造成IO卡顿的重要原因，但这也是保证内存中不会存在过量脏数据的保护机制。

大数据量项目中的感触：

1)     如果写入量巨大，不能期待系统缓存的自动回刷机制，最好采用应用层调用fsync或者sync。如果写入量大，甚至超过了系统缓存自动刷回的速度，就有可能导致系统的脏页率超过/proc/sys/vm/dirty_ratio， 这个时候，系统就会阻塞后续的写操作，这个阻塞有可能有5分钟之久，是我们应用无法承受的。因此，一种建议的方式是在应用层，在合适的时机调用fsync。

2)     对于关键性能，最好不要依赖于系统cache的作用，如果对性能的要求比较高，最好在应用层自己实现cache，因为系统cache受外界影响太大，说不定什么时候，系统cache就被冲走了。

3)     在logic设计中，发现一种需求使用系统cache实现非常合适，对于logic中的高楼贴，在应用层cache实现非常复杂，而其数量又非常少，这部分请求，可以依赖于系统cache发挥作用，但需要和应用层cache相配合，应用层cache可以cache住绝大部分的非高楼贴的请求，做到这一点后，整个程序对系统的io就主要在高楼贴这部分了。这种情况下，系统cache可以做到很好的效果。

另一种思路是，先调用fdatasync，再调用fsync，来降低堵塞时间。fdatasync某些场景是不需要刷metadata的，但是某些场景依旧需要刷metadata(例如涉及到文件大小的改变)，所以这个解决办法并不适用于所有场景。

最终的思路都是让刷metadata尽量快，尽量避免刷metadata时其data还没有刷到磁盘，等待刷其data的时间。

在某些应用场景，例如使用cgroup限制了IOPS的场景，需要非常注意。(这种场景下，内核进程刷脏页就非常有效，因为一般不限制内核进程的IOPS)。

ext4支持使用额外的journal设备来存储journal，如果你确定要这么做的话，建议使用IOPS性能好的块设备作为journal设备。mkfs.ext4时可以指定journal设备。

**二、**  **jbd2(journal block device)**

**2.1** **基本思想**

Ext4文件系统的日志功能是通过jdb2实现的，自从Linux系统引入了Ext4文件系统了，就有一个jbd2为之服务，其实jbd2也可以为其它的文件系统服务，但是目前来说只有Ext4和OCFS2文件系统用它。JBD2作用的原理是在Ext4文件系统把数据提交到驱动前先调用它，JBD2 根据系统的不同设置来完成数据或是操作的备份后，再让Ext4系统提交数据，当文件系统把数据写入了设备后，就通过JBD2把刚才数据或是操作备份删除，这样来保证数据的一致性。作用的流程如下：

![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image012.jpg)

如果没有jdb2，ext4的流程为红线表示。图中1-5阶段含义分别为：

1)  在数据准备提交的时候，先由JBD2根据系统设置的不同（writeback, ordered, journal）把对数据的操作写入备份，如果在这之前或是还没写完的时候系统发生了故障（如断电），那么在系统下次完整性检查时就把这些日志数据删除，而不对文件系统做任何改变。

2)  如果到这一步，说明备份数据已经写完，在以后的任何一步发生故障，系统都能根据日志把完整的备份数据写到相应的文件系统里去。当然只能保证一个原子操作完整性。假如你在拷贝一个600M的电影，一个原子操作是10M，那么JBD2系统只能保证下在提交的10M的数据的完成性，但不能保证这个电影的完整性。

**3)** 如果到这一步发生故障，那么数据还没有写入设备，或是没有完全写入设备，那么下次进行完整性检查时就会把数据补全，然后执行4-5两个。4、5也一样的，不过起点不一样，终点都一样的，所谓的殊途同归。

**2.2** **基本特点**

1)  将对文件系统的某些操作抽象成原子操作

所谓原子操作，就是内部不再分割的操作，该操作要么完全完成，要么根本没有执行，不存在部分完成的状态。

那么，什么样的操作可以看成对文件系统的原子操作呢？往一个磁盘文件中追加写入1MB 字节可以看成一个原子操作么？这个操作其实比较大， 因为要写 1MB 的数据， 要为文件分配 1024 个磁盘块，同时还要分配若干个索引块，也会涉及到很多的磁盘块位图、块组的读写， 非常复杂，时间也会比较长， 中间出问题的机会就比较多， 所以不适宜看做一个原子操作。

那么什么样的操作可以看成对文件系统的原子操作呢？比如说为文件分配一个磁盘块，就看成一个原子操作就比较合适。分配一个磁盘块，可能需要修改一个 inode 块、一个磁盘块位图、 最多三个间接索引块、块组块、超级块，一共最多 7 个磁盘块。将分配一个磁盘块看成一个原子操作， 意味着上述修改 7 个磁盘块的操作要么都成功， 要么都失败，不可能有第三种状态。

2)  将若干个原子操作组合成一个事务

实现日志文件系统时， 可以将一个原子操作就作为一个事务来处理， 但是这样实现的效率比较低。于是 ext3 中将若干个原子操作组合成一个事务，对磁盘日志以事务为单位进行管理，以提高读写日志的效率。

3)  在磁盘上单独划分一个日志空间

日志，在这里指的是磁盘上存储事务数据的那个地方， 即若干磁盘块。 它可以以一个单独的文件形式存，也可以由文件系统预留一个 inode 和一些磁盘块，也可以是一个单独的磁盘分区。总之，就是磁盘上存储事务数据的那个地方。

提到日志时， 可能还有另外一种含义， 就是指它是一种机制， 用于管理内存中的缓冲区、事务、磁盘日志数据读写等等所有这一切，统称为日志。读者注意根据上下文进行区分。 

4)  将内存中事务的数据写到日志中

文件系统可以选择定期 （每隔 5 秒， 或用户指定的时间间隔） 或者立即将内存中的事务数据写到磁盘日志上， 以备发生系统崩溃后可以利用日志中的数据恢复， 重新使文件系统保持一致的状态。这个间隔时间的选取，要注意性能的平衡。时间间隔越短， 文件系统丢失的数据可能性就越少，一致性的时间点就越新，但是 IO 负担就越重，很可能就会影响系统的性能。反过来，时间间隔越大，文件系统丢失的数据可能就越多，一致性的时间点就越旧，但是 IO 负

担就比较轻，不太会影响系统的性能。

5)  崩溃吧，然后我们从日志中恢复数据

我们不期望崩溃， 但是崩溃总会发生的， 如果发生了， 那就直面惨淡的人生吧！ 重新挂载分区时， 会根据日志中记录的数据， 逐一将其写回到磁盘原始位置上， 以保证文件系统的一致性。起死回生的奇迹发生了。jbd2 的思想就是原来内核读写磁盘的逻辑保持不变，但是对于影响文件系统一致性的数据块即元数据块， ，及时地写到磁盘上的日志空间中去。这样，即使系统崩溃了，也能从日志中恢复数据，确保文件系统的一致性。

**2.3**  **基本概念**

1)  buffer_head

buffer_head 是内核一个用于管理磁盘缓冲区的数据结构。根据局部性原理，磁盘上的数据进入内存后一般都是存放在磁盘缓冲区中的，以备将来重复读写。所以说，一个buffer_head 就会对应一个文件系统块，即对应一个磁盘块。

2)  元数据块

笼统地， 可以将一个文件系统内的块分为两种，一种是对文件系统的一致性有重要影响的、用于文件系统管理的磁盘块，称之为元数据块，包括超级块、磁盘位图块、inode 位图块、索引块、块组描述符块等等；另一种是存放文件数据的，称之为数据块。因为元数据块对文件系统的一致性有至关重要的影响， 故 jbd2 主要处理元数据块。

3)  handle

上面提到的原子操作， jbd2 中用 handle 来表示。 一个 handle 代表针对文件系统的一次原子操作。这个原子操作要么成功，要么失败，不会出现中间状态。在一个 handle 中，可能会修改若干个缓冲区，即 buffer_head。

4)  transaction

jbd2 为了提高效率，将若干个 handle 组成一个事务，用 transaction 来

表示。对日志读写来说，都是以 transaction 为单位的。在处理日志数据时，transaction 具有原子性， 即恢复时， 如果一个 transaction 是完整的，其中包含的数据就可用于文件系统的恢复，否则，忽略不完整的 transaction。 

5)  journal

journal 在英文中有“日志”之意，在 jbd2 中 journal 既是磁盘上日志空间的代表，又起到管理内存中为日志机制而创建的 handle、transaction 等数据结构的作用，可以说是整个日志机制的代表。

6)  commit

所谓提交，就是把内存中 transaction 中的磁盘缓冲区中的数据写到磁盘的日志空间上。注意，jbd2 是将缓冲区中的数据另外写一份，写到日志上，原来的 kernel 将缓冲区写回磁盘的过程并没有改变。内存中，transaction 是可以有若干个的，而不是只有一个。transaction 可分为三种，一种是已经 commit 到磁盘日志中的，它们正在进行 checkpoint 操作；第二种是正在将数据提交到日志的 transaction；第三种是正在运行的 transaction。 正在运行的 transaction管理随后发生的 handle，并在适当时间 commit 到磁盘日志中。注意正在运行的 transaction最多只可能有一个，也可能没有，如果没有，则 handle 提出请求时，则会按需要创建一个正在运行的 transaction。

7)  checkpoint

当一个 transaction 已经 commit，那么，是不是在内存中它就没有用了呢？好像是这样，因为其中的数据已经写到磁盘日志中了。 但是实际上不是这样的。 主要原因是磁盘日志是个有限的空间，比如说 100MB， 如果一直提交 transaction，很快就会占满，所以日志空间必须复用。其实与日志提交的同时，kernel 也在按照自己以前的方式将数据写回磁盘。试想，如果一个 transaction 中包含的所有磁盘缓冲区的数据都已写回到磁盘的原来的位置上 （不是日志中，而是在磁盘的原来的物理块上） ，那么，该 transaction 就没有用了，可以被删除了，该transaction 在磁盘日志中的空间就可以被回收，进而重复利用了。

8)  revoke

假设有一个缓冲区，对应着一个磁盘块， 内核多次修改该缓冲区， 于是磁盘日志中就会有该缓冲区的若干个版本的数据。 假设此时要从文件中删除该磁盘块， 那么，一旦包含该删除操作的 transaction 提交， 那么， 再恢复时， 已经存放在磁盘日志中的该磁盘块的若干个版本的数据就不必再恢复了， 因为到头来还是要删除的。 revoke 就是这样一种加速恢复速度的方法。当本 transaction 包含删除磁盘块操作时，就会在磁盘日志中写一个 revoke 块，该块中包含<被 revoked 的块号 blocknr， 提交的 transaction 的 ID>， 表示恢复时， 凡是 transaction ID 小于等于 ID 的所有写磁盘块 blocknr 的操作都可以取消了，不必进行了。

9)  recover

加入日志机制后，一旦系统崩溃， 重新挂载分区时， 就会检查该分区上的日志是否需要恢复。 如果需要， 则依次将日志空间的数据写回磁盘原始位置， 则文件系统又重新处于一致状态了。

10) kjournald

日志的提交操作是由一个内核线程实现的，该线程称为 kjournald。该内核线程平时一直在睡眠，直到有进程主动唤醒它，或者是定时器时间到了（一般为每隔 5 秒） 。被唤醒后它就进行事务的提交操作。

**3.1** **日志在磁盘中布局**

日志中是以文件系统块为单位组织的， 总体上可分为两种，一种是数据块， 另一种是描述块。这里所谓的数据块， 即表示块里存放的是文件系统数据的块， 既可以是文件系统的元数据块，又可以是文件系统的数据块。 而这里的描述块，则是日志中特有的、为日志机制特别设置的、起组织管理日志数据作用的块。

存储一共有五种块，定义如下

\#define JFS_DESCRIPTOR_BLOCK 1 

\#define JFS_COMMIT_BLOCK 2 

\#define JFS_SUPERBLOCK_V1  3 

\#define JFS_SUPERBLOCK_V2 4 

\#define JFS_REVOKE_BLOCK 5

1)  超级块 JFS_SUPERBLOCK_V1、JFS_SUPERBLOCK_V2 

超级块一共有两种格式，以前使用第一种，现在都使用第二种了。

journal_superblock_t 结构就是超级块内存中的表示。日志中超级块起的作用与文件系统中超级块的作用是类似的， 都是用于组织管理一段磁盘空间。 所以日志从一方面看， 是一个文件， 但是从另一个方面看，又可看做一个小的文件

系统。

![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image014.jpg)

![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image016.jpg)

2)  描述符块 JFS_DESCRIPTOR_BLOCK 

日志存放的块的内容，来自于原来的文件系统，并且最终也要写回到原来的文件系统。比如说，原来一个 ext4 文件系统的第 1078 块是一个元数据块，进入内核，被修改了，然后先写到日志中的第 37 块中。假设此时系统崩溃了，那么恢复时，我们要把日志中的第 37块中的数据，写回到文件系统的第 1078 块中。也就是说，37 与 1078 之间，有一种对应关系， 用于指示恢复时，要把日志中的哪一块写回到文件系统中的哪一块。 这种对应关系， 就记录在描述符块中。

在描述符块中，使用一个 journal_block_tag_t 结构来描述日志中的一个块与磁盘上的一个块的对应关系的。一个 transaction 日志中会对应三部分， 第一部分是一个描述符块， 第二部分是若干个数据块，第三部分是一个提交块。描述符块的开头是一个 journal_header_t 结构，用于表示本块是一个描述块。然后是若干个索引项， 用于表示描述符块之后的数据块与磁盘原始块的对应关系。

![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image018.jpg)

注意：

**a)** **UUID** **：**

第一个索引项包含一个 128 位的 UUID，来自于该块所属的文件系统。为了节省空间，从第二个索引项开始， 如果所属的文件系统没有变化， 则在 journal_block_tag_t-> t_flags中设置一个标志 JFS_FLAG_SAME_UUID，这样本索引项中就不用再保存 UUID 了。

b)  如何表示日志块

描述符块的主要作用是描述一个日志块对应哪个磁盘块，其中，目标磁盘块存在journal_block_tag_t->t_blocknr 中，而日志块，则是根据索引项的序号确定的。即第一个索引项 （编号为 0），描述的是日志中在本描述符块之后的第一个数据块，即表格 2中的数据块 0。

c)  最后一个索引项

一个 transaction 对应一个描述符块， 但是 transaction 中的缓冲区的个数是不定的， 不一定能正好占满一个描述符块。JFS_FLAG_LAST_T AG 标志表示这是本描述符块中的最后一个有效的索引项。

d)  被转义的数据块

描述符块之后就是数据块了。数据块中的内容是任意的，可能某个数据块的开头的 4个字节正好是 0xc03b3998U ， 即 JFS_MAGIC_NUMBER， 那怎么办呢？这个块明明是个普通的数据块， 但是因为开头的偶然的 4 个字节， 就会被 jbd 误认为是一个具有特殊含义的描述块了。 

解决方法类似于 shell 中的转义字符的概念。当要往日志中写一块普通的数据块时，如果发现其开头的 4 个字节恰好是 0xc03b3998U，则将该 4 个字节改写成 0，并且在描述符块的索引项中设置 JFS_FLAG_ESCAPE，用于表示该日志块中的数据是被转义过的。恢复时，如果在索引项中发现了 JFS_FLAG_ESCAPE 标志，则重新把该块的前 4 个字节设置成 0xc03b3998U，然后写回到磁盘的原始位置上。

日志上的描述块的示意图如下：

![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image020.jpg)

3)  提交块 JFS_COMMIT_BLOCK

提交块用于表明本 transaction 是一个完整的 transaction，恢复时可以使用。每个 transaction 提交时， 当把所有的数据都写完了之后， 就会写入一个提交块， 表示我们已经写入了一个完整的 transaction。其实提交块中就只有一个 journal_header_t 结构。

![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image022.jpg)

4)  取消块 JFS_REVOKE_BLOCK

取消块是为了加快恢复速度而设置的，里面保存着一个 transaction 

ID 和一些块号，用于表示恢复时这些块不用被恢复。取消块中只记录块号。

![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image024.jpg)

5)  日志布局

从内存中看，一个日志由日志超级块和一系列事务组成。参见图表 4 内存中日志结构示意图：

![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image026.jpg)

从物理角度来看， 日志在磁盘上就是由一些磁盘块组成的， 这些磁盘块从日志的角度分析看，可能是超级块、描述符块、数据块、提交块、取消块等。日志块布局示意图：

![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image028.jpg)

其中红色背景的块表示日志的描述块，白色背景的块表示日志的数据块。

三、   代码结构

ext4文件系统的相关代码位于内核代码目录fs/ext4文件夹下，其起始文件是super.c，在该文件的底端，定义了初始化和退出函数：

![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image030.jpg)

在该函数的最后注册了ext4文件系统：

![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image032.jpg)

其中ext4_fs_type结构体变量定义如下：

![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image034.jpg)

在系统启动挂载ext4文件系统时，最终会调用ext4_mount函数。

![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image036.jpg)

最终会调用ext4_fill_super函数，该函数主要作用就是从emmc中读取文件系统超级块来初始化内存中的相关结构体。

jbd2相关代码位于fs/jbd2文件夹下。其起始文件是journal.c。文件最后定义了初始化和退出函数：

![img](file:///C:\Users\luxing3\AppData\Local\Temp\msohtmlclip1\01\clip_image038.jpg)

 

## 问题

[Htree](http://zh.wikipedia.org/w/index.php?title=Htree&action=edit&redlink=1)（一种特殊的[B树](http://zh.wikipedia.org/wiki/B树)）B树  B＋树等具体内容？是啥